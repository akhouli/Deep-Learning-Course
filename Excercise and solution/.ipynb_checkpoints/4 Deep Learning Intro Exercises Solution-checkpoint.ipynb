{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The [Pima Indians dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) is a very famous dataset distributed by UCI and originally collected from the National Institute of Diabetes and Digestive and Kidney Diseases. It contains data from clinical exams for women age 21 and above of Pima indian origins. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n",
    "\n",
    "It has the following features:\n",
    "\n",
    "- Pregnancies: Number of times pregnant\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "- BloodPressure: Diastolic blood pressure (mm Hg)\n",
    "- SkinThickness: Triceps skin fold thickness (mm)\n",
    "- Insulin: 2-Hour serum insulin (mu U/ml)\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "- DiabetesPedigreeFunction: Diabetes pedigree function\n",
    "- Age: Age (years)\n",
    "\n",
    "The last colum is the outcome, and it is a binary variable.\n",
    "\n",
    "In this first exercise we will explore it through the following steps:\n",
    "\n",
    "1. Load the ..data/diabetes.csv dataset, use pandas to explore the range of each feature\n",
    "- For each feature draw a histogram. Bonus points if you draw all the histograms in the same figure.\n",
    "- Explore correlations of features with the outcome column. You can do this in several ways, for example using the `sns.pairplot` we used above or drawing a heatmap of the correlations.\n",
    "- Do features need standardization? If so what stardardization technique will you use? MinMax? Standard?\n",
    "- Prepare your final `X` and `y` variables to be used by a ML model. Make sure you define your target variable well. Will you need dummy columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.hist(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad\\Anaconda3\\envs\\ztdl\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Ahmad\\Anaconda3\\envs\\ztdl\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(df.drop('Outcome', axis=1))\n",
    "y = df['Outcome'].values\n",
    "y_cat = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Build a fully connected NN model that predicts diabetes. Follow these steps:\n",
    "\n",
    "1. Split your data in a train/test with a test size of 20% and a `random_state = 22`\n",
    "- define a sequential model with at least one inner layer. You will have to make choices for the following things:\n",
    "    - what is the size of the input?\n",
    "    - how many nodes will you use in each layer?\n",
    "    - what is the size of the output?\n",
    "    - what activation functions will you use in the inner layers?\n",
    "    - what activation function will you use at output?\n",
    "    - what loss function will you use?\n",
    "    - what optimizer will you use?\n",
    "- fit your model on the training set, using a validation_split of 0.1\n",
    "- test your trained model on the test data from the train/test split\n",
    "- check the accuracy score, the confusion matrix and the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat,\n",
    "                                                    random_state=22,\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(Adam(lr=0.05),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*8 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 552 samples, validate on 62 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.5662 - acc: 0.7029 - val_loss: 0.6389 - val_acc: 0.7903\n",
      "Epoch 2/20\n",
      " - 0s - loss: 0.5054 - acc: 0.7536 - val_loss: 0.6086 - val_acc: 0.7742\n",
      "Epoch 3/20\n",
      " - 0s - loss: 0.4520 - acc: 0.7880 - val_loss: 0.5351 - val_acc: 0.7903\n",
      "Epoch 4/20\n",
      " - 0s - loss: 0.4520 - acc: 0.7663 - val_loss: 0.7687 - val_acc: 0.7742\n",
      "Epoch 5/20\n",
      " - 0s - loss: 0.4751 - acc: 0.7808 - val_loss: 0.4940 - val_acc: 0.8065\n",
      "Epoch 6/20\n",
      " - 0s - loss: 0.4419 - acc: 0.7917 - val_loss: 0.6139 - val_acc: 0.7419\n",
      "Epoch 7/20\n",
      " - 0s - loss: 0.4469 - acc: 0.7844 - val_loss: 0.5614 - val_acc: 0.7581\n",
      "Epoch 8/20\n",
      " - 0s - loss: 0.4407 - acc: 0.7880 - val_loss: 0.5925 - val_acc: 0.7419\n",
      "Epoch 9/20\n",
      " - 0s - loss: 0.4470 - acc: 0.7917 - val_loss: 0.5047 - val_acc: 0.7903\n",
      "Epoch 10/20\n",
      " - 0s - loss: 0.4690 - acc: 0.7862 - val_loss: 0.5981 - val_acc: 0.7742\n",
      "Epoch 11/20\n",
      " - 0s - loss: 0.4704 - acc: 0.7464 - val_loss: 0.5587 - val_acc: 0.8065\n",
      "Epoch 12/20\n",
      " - 0s - loss: 0.4172 - acc: 0.7971 - val_loss: 0.6441 - val_acc: 0.7581\n",
      "Epoch 13/20\n",
      " - 0s - loss: 0.4254 - acc: 0.7826 - val_loss: 0.6580 - val_acc: 0.7419\n",
      "Epoch 14/20\n",
      " - 0s - loss: 0.4134 - acc: 0.7953 - val_loss: 0.5579 - val_acc: 0.7742\n",
      "Epoch 15/20\n",
      " - 0s - loss: 0.3970 - acc: 0.7935 - val_loss: 0.7192 - val_acc: 0.7742\n",
      "Epoch 16/20\n",
      " - 0s - loss: 0.4027 - acc: 0.8043 - val_loss: 0.5924 - val_acc: 0.7581\n",
      "Epoch 17/20\n",
      " - 0s - loss: 0.4274 - acc: 0.7844 - val_loss: 0.6098 - val_acc: 0.7581\n",
      "Epoch 18/20\n",
      " - 0s - loss: 0.4035 - acc: 0.8025 - val_loss: 0.6881 - val_acc: 0.7258\n",
      "Epoch 19/20\n",
      " - 0s - loss: 0.3794 - acc: 0.7971 - val_loss: 0.6921 - val_acc: 0.7258\n",
      "Epoch 20/20\n",
      " - 0s - loss: 0.3764 - acc: 0.8062 - val_loss: 0.8180 - val_acc: 0.7581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x201aeca08d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.649351\n",
       "1    0.350649\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test_class).value_counts() / len(y_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207792207792207"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.91      0.81       100\n",
      "           1       0.69      0.37      0.48        54\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       154\n",
      "   macro avg       0.71      0.64      0.65       154\n",
      "weighted avg       0.71      0.72      0.69       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91,  9],\n",
       "       [34, 20]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Compare your work with the results presented in [this notebook](https://www.kaggle.com/futurist/d/uciml/pima-indians-diabetes-database/pima-data-visualisation-and-machine-learning). Are your Neural Network results better or worse than the results obtained by traditional Machine Learning techniques?\n",
    "\n",
    "- Try training a Support Vector Machine or a Random Forest model on the exact same train/test split. Is the performance better or worse?\n",
    "- Try restricting your features to only 4 features like in the suggested notebook. How does model performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy score: 0.734\n",
      "Confusion Matrix:\n",
      "[[90 10]\n",
      " [31 23]]\n",
      "\n",
      "================================================================================\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy score: 0.721\n",
      "Confusion Matrix:\n",
      "[[89 11]\n",
      " [32 22]]\n",
      "\n",
      "================================================================================\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy score: 0.708\n",
      "Confusion Matrix:\n",
      "[[87 13]\n",
      " [32 22]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad\\Anaconda3\\envs\\ztdl\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Ahmad\\Anaconda3\\envs\\ztdl\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "for mod in [RandomForestClassifier(), SVC(), GaussianNB()]:\n",
    "    mod.fit(X_train, y_train[:, 1])\n",
    "    y_pred = mod.predict(X_test)\n",
    "    print(\"=\"*80)\n",
    "    print(mod)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Accuracy score: {:0.3}\".format(accuracy_score(y_test_class,\n",
    "                                                         y_pred)))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test_class, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "[Tensorflow playground](http://playground.tensorflow.org/) is a web based neural network demo. It is really useful to develop an intuition about what happens when you change architecture, activation function or other parameters. Try playing with it for a few minutes. You don't nee do understand the meaning of every knob and button in the page, just get a sense for what happens if you change something. In the next chapter we'll explore these things in more detail.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
